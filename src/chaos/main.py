# Store/src/chaos/main.py
# @ai-rules:
# 1. [Single endpoint]: All mutations go through POST /api/settings with ChaosSettings Pydantic model.
# 2. [Memory cap]: MEMORY_CAP_MB must stay aligned with container limit (320Mi) minus base overhead (~120Mi).
# 3. [HTTP load]: cpu_threads (1-8) maps to HTTP concurrency. Mixed traffic: 60% GET, 25% POST, 15% DELETE.
#    Chaos-created entities are tagged with "chaos-" prefix and cleaned up on stop.
# 4. [No Query import]: Old GET endpoints were removed. Do not re-add fastapi.Query.
"""
Darwin Chaos Controller - Fault injection API.

Provides endpoints to inject chaos into the Darwin Store:
- HTTP load testing (concurrent GET/POST/DELETE requests to backend)
- Memory pressure
- Artificial latency
- Error injection

Runs on port 9000, separate from Store (port 8080).
"""

import os
import asyncio
import threading
import logging
import random
import uuid
from collections import deque
from datetime import datetime, timezone
from pathlib import Path
from dataclasses import asdict
from typing import Optional, List

import httpx
from fastapi import FastAPI
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse
from pydantic import BaseModel, Field

# Use absolute import from src package
# When running as `uvicorn src.chaos.main:app`, src is the root package
from src.app.chaos_state import get_chaos, set_chaos, reset_chaos
from src.chaos.db import init_db, close_db, insert_report, list_reports, get_latest_report

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Max safe chaos memory: container limit (320Mi) minus base overhead (~120Mi)
MEMORY_CAP_MB = 200

# Backend URL for HTTP load testing (resolved via K8s service DNS)
BACKEND_URL = os.getenv("BACKEND_URL", "http://darwin-store-backend:8080")

# GET endpoints to cycle through for read load
_GET_ENDPOINTS = ["/products", "/orders", "/customers", "/health"]

# Weighted actions for mixed traffic: (action, weight)
# 60% GET (reads), 25% POST (creates), 15% DELETE (cleanup)
_LOAD_ACTIONS = ["GET", "POST", "DELETE"]
_LOAD_WEIGHTS = [60, 25, 15]


class ChaosSettings(BaseModel):
    """Request body for POST /api/settings."""
    cpu_threads: Optional[int] = Field(None, ge=0, le=8)
    memory_mb: Optional[int] = Field(None, ge=0, le=MEMORY_CAP_MB)
    latency_ms: Optional[int] = Field(None, ge=0, le=30000)
    error_rate: Optional[float] = Field(None, ge=0.0, le=1.0)
    reset: Optional[bool] = None


# HTTP load task management
_load_tasks: list[asyncio.Task] = []
_load_stop_event: Optional[asyncio.Event] = None

# Memory burn management
_memory_buffer: list[bytearray] = []  # Holds allocated memory
_memory_lock = threading.Lock()


def _make_product_body() -> dict:
    """Generate a chaos-tagged product payload."""
    tag = uuid.uuid4().hex[:8]
    return {
        "name": f"chaos-{tag}",
        "price": round(random.uniform(1.0, 100.0), 2),
        "stock": random.randint(1, 50),
        "sku": f"CHAOS-{tag}",
        "description": "Auto-generated by chaos controller",
    }


def _make_customer_body() -> dict:
    """Generate a chaos-tagged customer payload."""
    tag = uuid.uuid4().hex[:8]
    return {
        "name": f"chaos-{tag}",
        "email": f"chaos-{tag}@example.com",
        "company": "Chaos Corp",
        "phone": "555-0000",
    }


async def _http_load_loop(
    worker_id: int,
    stop_event: asyncio.Event,
    created_products: deque,
    created_customers: deque,
):
    """
    Send weighted GET/POST/DELETE requests to the backend until stopped.

    Traffic mix: ~60% GET (reads), ~25% POST (creates), ~15% DELETE.
    Created entities are tracked in shared deques for cleanup on stop.
    """
    logger.info(f"HTTP load worker {worker_id} started -> {BACKEND_URL}")
    idx = worker_id  # Offset for GET endpoint cycling
    async with httpx.AsyncClient(base_url=BACKEND_URL, timeout=5.0) as client:
        while not stop_event.is_set():
            action = random.choices(_LOAD_ACTIONS, weights=_LOAD_WEIGHTS, k=1)[0]
            try:
                if action == "GET":
                    endpoint = _GET_ENDPOINTS[idx % len(_GET_ENDPOINTS)]
                    await client.get(endpoint)
                    idx += 1

                elif action == "POST":
                    # Alternate between products and customers
                    if random.random() < 0.5:
                        resp = await client.post("/products", json=_make_product_body())
                        if resp.status_code == 201:
                            data = resp.json()
                            created_products.append(data["id"])
                    else:
                        resp = await client.post("/customers", json=_make_customer_body())
                        if resp.status_code == 201:
                            data = resp.json()
                            created_customers.append(data["id"])

                elif action == "DELETE":
                    # Delete a previously created entity; fall back to GET if none
                    if created_products and random.random() < 0.5:
                        pid = created_products.popleft()
                        await client.delete(f"/products/{pid}")
                    elif created_customers:
                        cid = created_customers.popleft()
                        await client.delete(f"/customers/{cid}")
                    else:
                        # Nothing to delete yet; do a GET instead
                        endpoint = _GET_ENDPOINTS[idx % len(_GET_ENDPOINTS)]
                        await client.get(endpoint)
                        idx += 1

            except Exception:
                pass  # Backend may be slow/down under load; keep hammering
            # Tiny yield to avoid starving the event loop
            await asyncio.sleep(0.01)
    logger.info(f"HTTP load worker {worker_id} stopped")


# Shared deques for tracking chaos-created entity IDs across workers
# No maxlen — all created IDs must be retained for cleanup on stop
_created_products: deque = deque()
_created_customers: deque = deque()


async def _cleanup_chaos_entities():
    """Sweep-delete remaining chaos-created products and customers.

    Two-phase cleanup:
    1. Fast path — delete tracked IDs from the in-memory deques.
    2. Full sweep — query the backend for any remaining chaos-* entities
       (catches items missed if the process restarted or deques were lost).
    """
    deleted = 0
    async with httpx.AsyncClient(base_url=BACKEND_URL, timeout=10.0) as client:
        # Phase 1: drain tracked IDs
        while _created_products:
            pid = _created_products.popleft()
            try:
                await client.delete(f"/products/{pid}")
                deleted += 1
            except Exception:
                pass
        while _created_customers:
            cid = _created_customers.popleft()
            try:
                await client.delete(f"/customers/{cid}")
                deleted += 1
            except Exception:
                pass

        # Phase 2: full sweep for any remaining chaos-* entities
        for resource in ("products", "customers"):
            try:
                resp = await client.get(f"/{resource}")
                if resp.status_code == 200:
                    for entity in resp.json():
                        if entity.get("name", "").startswith("chaos-"):
                            try:
                                await client.delete(f"/{resource}/{entity['id']}")
                                deleted += 1
                            except Exception:
                                pass
            except Exception:
                logger.warning(f"Cleanup: failed to sweep /{resource}")

    if deleted:
        logger.info(f"Cleanup: deleted {deleted} chaos entities")


async def _stop_load_tasks():
    """Signal all HTTP load workers to stop, wait, then clean up chaos entities."""
    global _load_tasks, _load_stop_event
    if _load_stop_event is not None:
        _load_stop_event.set()
    for task in _load_tasks:
        if not task.done():
            task.cancel()
            try:
                await task
            except asyncio.CancelledError:
                pass
    _load_tasks = []
    _load_stop_event = None
    # Sweep-delete any remaining chaos entities
    await _cleanup_chaos_entities()


# Create FastAPI app
app = FastAPI(
    title="Darwin Chaos Controller",
    description="Fault injection API for Darwin demos",
    version="1.0.0"
)


@app.get("/")
async def index():
    """Serve the Chaos UI."""
    static_dir = Path(__file__).parent / "static"
    index_file = static_dir / "index.html"
    if index_file.exists():
        return HTMLResponse(content=index_file.read_text())
    return HTMLResponse(content="<h1>Chaos Controller</h1><p>Static files not found</p>")


@app.get("/api/status")
async def get_status():
    """Get current chaos state."""
    chaos = get_chaos()
    active_workers = sum(1 for t in _load_tasks if not t.done())
    memory_chunks = len(_memory_buffer)
    return {
        "chaos": asdict(chaos),
        "cpu_threads_active": active_workers,
        "memory_chunks": memory_chunks,
        "memory_allocated_mb": memory_chunks * 10
    }


@app.post("/api/settings")
async def update_settings(body: ChaosSettings = None):
    """
    Update chaos injection settings.

    Accepts a JSON body validated by ChaosSettings model.
    All fields are optional; only provided fields are applied.
    Pydantic enforces ranges (e.g., cpu_threads 0-8, memory_mb 0-MEMORY_CAP_MB).
    """
    global _load_tasks, _load_stop_event, _memory_buffer

    if body is None:
        return {"status": "current", "settings": asdict(get_chaos())}

    result = {}

    # Reset all
    if body.reset:
        await _stop_load_tasks()
        with _memory_lock:
            _memory_buffer.clear()
        reset_chaos()
        logger.info("Settings reset to defaults")
        return {"status": "reset", "settings": asdict(get_chaos())}

    # HTTP load (mapped from cpu_threads slider: 1-8 = concurrency level)
    if body.cpu_threads is not None:
        concurrency = body.cpu_threads
        await _stop_load_tasks()
        if concurrency > 0:
            set_chaos(cpu_threads=concurrency)
            _load_stop_event = asyncio.Event()
            for i in range(concurrency):
                task = asyncio.create_task(
                    _http_load_loop(i, _load_stop_event, _created_products, _created_customers)
                )
                _load_tasks.append(task)
            result["http_load"] = {"concurrency": concurrency, "active": True, "target": BACKEND_URL}
        else:
            set_chaos(cpu_threads=0)
            result["http_load"] = {"concurrency": 0, "active": False}

    # Memory
    if body.memory_mb is not None:
        mb = body.memory_mb
        with _memory_lock:
            _memory_buffer.clear()
            if mb > 0:
                chunk_size = 10 * 1024 * 1024
                for i in range(mb // 10):
                    try:
                        chunk = bytearray(chunk_size)
                        for j in range(0, len(chunk), 4096):
                            chunk[j] = i % 256
                        _memory_buffer.append(chunk)
                    except MemoryError:
                        break
            actual = len(_memory_buffer) * 10
            set_chaos(memory_load_mb=actual)
            result["memory_mb"] = actual

    # Latency
    if body.latency_ms is not None:
        set_chaos(latency_ms=body.latency_ms)
        result["latency_ms"] = body.latency_ms

    # Error rate
    if body.error_rate is not None:
        set_chaos(error_rate=body.error_rate)
        result["error_rate"] = body.error_rate

    logger.info(f"Settings updated: {result}")
    return {"status": "updated", "applied": result, "settings": asdict(get_chaos())}


# ---------------------------------------------------------------------------
# Test Reports API — stores post-deploy Playwright test results
# ---------------------------------------------------------------------------

class TestCase(BaseModel):
    """A single test case result."""
    name: str
    status: str  # "passed", "failed", "skipped"
    duration_ms: float = 0
    error: Optional[str] = None


class TestReport(BaseModel):
    """Payload from the post-deploy test runner job."""
    suite: str = "post-deploy"
    total: int = 0
    passed: int = 0
    failed: int = 0
    skipped: int = 0
    duration_ms: float = 0
    tests: List[TestCase] = []
    git_sha: Optional[str] = None
    image_tag: Optional[str] = None


class StoredReport(BaseModel):
    """A test report with server-assigned metadata."""
    id: str
    received_at: str
    suite: str
    total: int
    passed: int
    failed: int
    skipped: int
    duration_ms: float
    tests: List[TestCase]
    git_sha: Optional[str] = None
    image_tag: Optional[str] = None


@app.on_event("startup")
async def startup_db():
    """Initialize DB for test report persistence."""
    init_db()


@app.on_event("shutdown")
async def shutdown_db():
    """Close DB pool on shutdown."""
    close_db()


@app.post("/api/test-reports", status_code=201)
async def post_test_report(body: TestReport):
    """Receive a test report from the post-deploy job.

    Persists to PostgreSQL with strict FIFO: only the 7 most recent reports
    are kept in the database. Falls back to in-memory if DB is unavailable.
    """
    report_dict = {
        "id": uuid.uuid4().hex[:12],
        "received_at": datetime.now(timezone.utc).isoformat(),
        "suite": body.suite,
        "total": body.total,
        "passed": body.passed,
        "failed": body.failed,
        "skipped": body.skipped,
        "duration_ms": body.duration_ms,
        "tests": [t.model_dump() for t in body.tests],
        "git_sha": body.git_sha,
        "image_tag": body.image_tag,
    }
    insert_report(report_dict)
    logger.info(f"Test report received: {body.passed}/{body.total} passed (id={report_dict['id']})")
    return {"status": "stored", "id": report_dict["id"]}


def _group_by_deployment_run(reports: list) -> list:
    """Group flat report list into deployment runs keyed by git_sha."""
    runs: dict = {}
    run_order: list = []
    for r in reports:
        key = r.get("git_sha") or r.get("id")
        if key not in runs:
            runs[key] = {
                "git_sha": r.get("git_sha"),
                "image_tag": r.get("image_tag"),
                "received_at": r["received_at"],
                "suites": [],
                "total": 0,
                "passed": 0,
                "failed": 0,
                "skipped": 0,
            }
            run_order.append(key)
        run = runs[key]
        run["suites"].append(r)
        run["total"] += r["total"]
        run["passed"] += r["passed"]
        run["failed"] += r["failed"]
        run["skipped"] += r["skipped"]
    return [runs[k] for k in run_order]


@app.get("/api/test-reports")
async def list_test_reports():
    """Return the last 7 deployment runs with their suite reports grouped."""
    reports = list_reports()
    return _group_by_deployment_run(reports)


@app.get("/api/test-reports/latest")
async def get_latest_test_report():
    """Get the most recent test report."""
    report = get_latest_report()
    if report is None:
        return {"status": "no_reports"}
    return report


# Mount static files (must be after routes)
static_dir = Path(__file__).parent / "static"
if static_dir.exists():
    app.mount("/static", StaticFiles(directory=str(static_dir)), name="static")
